---
title: "Pre-test results exploration"
author: "Sophie Arana"
output:
  html_document:
    code_folding: hide
    df_print: paged
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
df_print: paged
---

```{r,echo=FALSE, message=FALSE}
library(plotly)
library(ggplot2)
library(reshape)
library(RColorBrewer)
library(data.table)
library(plyr)
library(dplyr)
library(MASS)
select <- dplyr::select
library(kableExtra)
library(knitr)
library(shiny)
library(DT)
library(lme4)
options(knitr.table.format="html")
#Create color scheme
pastel_colors = brewer.pal(8, "Pastel1")
set_colors = brewer.pal(8, "Set1")
```

# Loading in data
All following graphs and statistics are based on the results of the Limesurvey online pre-test. 40 participants filled out the online pre-test, of which 20 saw the same sentences and each participant saw them in a different pseudo-randomized order.

```{r}
stimuli         <- read.csv(file="stimuli/stimuli_forpretest.csv",head=TRUE,sep=";",na.strings = c("","NAN"), stringsAsFactors = FALSE,encoding="UTF-8")
stimuli$fullID  <- as.character(interaction(stimuli[,c(13,3)],sep = ""))
stimuli$Condition <- as.factor(stimuli$Condition)
colnames(stimuli) <- c("Type","Pair","ID","Det.","N0","Verb","Det.2","N1","Prep.","Det.3","Adj.","N2","Attachment","Unambiguous","verb_number","fullID")
#load response file
file_names = list.files(pattern="^results",path = "allresponses",full.names = TRUE)
temp <- lapply(file_names,read.csv,sep=",",na.strings = c("","NAN"), stringsAsFactors = TRUE)
df.responses <- Reduce(function(x,y) merge(x,y,all=TRUE,sort=TRUE),temp)
#transpose and fix column names and classes
df.responses                <- setDT(df.responses,keep.rownames = TRUE)
colnames(df.responses)[1]   <- "subjects"
df.responses                <- Filter(function(x) !(all(x=="")), df.responses) #delete blank columns
```

Data was subsequently split into subject info (demographics etc.) and responses to items

```{r}
#Split all item specific data from subject specific data
ind_items   <- grep("[VN]A[0-9]",colnames(df.responses),value = TRUE)
df.subjectinfo <- df.responses
df.subjectinfo[,ind_items] <- NULL
df.responses  <- df.responses[,c("subjects",ind_items),with=FALSE]
```
```{r}
#reshape items so that each row contains info for items per subject (items repeat over rows)
ind             <- grep("^[VN]A[0-9]*$",colnames(df.responses),value = TRUE)
df.respAttach   <- df.responses[,c("subjects",ind),with=FALSE]
df.respAttach   <- melt(df.respAttach,id="subjects",value.name="response_attachment",variable.name='items')
ind             <- grep("^[VN]A[0-9]*Time$",colnames(df.responses),value = TRUE)
df.respTime     <- df.responses[,c("subjects",ind),with=FALSE]
df.respTime     <- melt(df.respTime,id="subjects",value.name="rt_attachment",variable.name='items')
ind             <- grep("\\.$",colnames(df.responses),value = TRUE)
df.respPAttach  <- df.responses[,c("subjects",ind),with=FALSE]
df.respPAttach  <- melt(df.respPAttach,id="subjects",value.name="rating_plausibility",variable.name='items')
ind             <- grep("P[VN]A[0-9]*Time$",colnames(df.responses),value = TRUE)
df.respPTime    <- df.responses[,c("subjects",ind),with=FALSE]
df.respPTime    <- melt(df.respPTime,id="subjects",value.name="rt_plausibility",variable.name='items')
df.responses    <- cbind(df.respAttach,df.respTime[,3],df.respPAttach[,3],df.respPTime[,3])
df.responses$hits <- as.numeric((grepl("N",df.responses$items) & df.responses$response_attachment == "Nomen") | 
                      (grepl("V",df.responses$items) & df.responses$response_attachment == "Verb"))
df.responses    <- df.responses %>%
                    mutate(attachment = ifelse(grepl("N",items),"Noun","Verb"))
```


## Summary Statistics
Age of participants:
```{r}
summary(df.subjectinfo$age)
```
Time taken to complete pre-test (in minutes):
```{r}
summary((df.subjectinfo$interviewtime)/60)
```
## 1. Outlier
Outlier subjects were identified according to three scores:  
1. Subjects who have more than one incorrect test item  
2. Subjects who have less than 60% correct for the unambiguous items  
3. Subjects who's average reaction time diverges extremely from the average

### 1a. Unambiguous sentences:
```{r,warning=FALSE,message=FALSE}
#average accuracy of unambiguous items
subset_unambiguous <- stimuli$fullID[(stimuli$Unambiguous==1)]
sentences_unambiguous <- stimuli[(stimuli$Unambiguous==1),4:12]
sentences_unambiguous
```

### 1b. Reaction Times
```{r,warning=FALSE,message=FALSE}
least_correct = 3
threshhold_acc = 0.60
avg_rts <- df.responses %>%
                        group_by(subjects) %>%
                        summarise(mean_rt = mean(rt_attachment,na.rm=TRUE)) 

outlier <- boxplot.stats(avg_rts$mean_rt)$out
p <- ggplot(avg_rts, aes(x=factor(0),y=mean_rt,subject=subjects)) +
    geom_boxplot() +
    geom_jitter(size = 2) +
    ggtitle("Average reaction times per subject across all items")
p <- ggplotly(p,tooltip ='subjects')
p

test_correct   <- df.subjectinfo %>%
                   select(subjects,starts_with("correct")) %>%
                      mutate(number_correct = rowSums(!is.na(.[,2:ncol(.)]))) %>%
                        select(subjects,number_correct)

avg_unambiguous <-df.responses %>%
                    filter(items %in% subset_unambiguous) %>%
                      group_by(subjects) %>%
                        summarise(acc_unambiguous = mean(hits,na.rm=TRUE))
```
### 1c. Summary table per subject
```{r,warning=FALSE,message=FALSE}
summ <- join_all(list(avg_rts,test_correct,avg_unambiguous),by='subjects')
DT::datatable(summ) %>% formatRound(columns=colnames(summ),digits=2)

```
```{r,warning=FALSE,message=FALSE}
outlier_indx <- avg_rts$subjects[which(round(avg_rts$mean_rt,digits=2) %in% round(outlier,digits=2))]
outlier_indx <- c(outlier_indx,summ$subjects[which(summ$number_correct < least_correct | summ$acc_unambiguous < threshhold_acc)])
outlier_indx <- unique(outlier_indx)
#Remove Outlier
df.responses <- df.responses %>%
                filter(!(subjects %in% outlier_indx))
``` 
  
Number of removed outliers: `r length(outlier_indx)`

## 2. Assess items
```{r,warning=FALSE,message=FALSE}
#Binomial distribution of probabilities if chance-performane is 50%
x <- seq(0,20,by = 1)
y <- dbinom(x,20,0.5)
plot(x,y,main="Probability distribution assuming per item probability of 50%",
      xlab="Number of correct answers per items (out of 20 answers)", ylab = "Probability")

```
  
### Reject stimuli with too few correct responses (less than 72% expected answers)
### 2a. Rejected items:
```{r,warning=FALSE,message=FALSE}
threshold = 0.72

acc_per_item <- df.responses %>%
                  group_by(items) %>%
                    summarise(acc = mean(hits,na.rm = TRUE), plaus = mean(rating_plausibility,na.rm = TRUE))
mean_accuracy <- acc_per_item %>%
                    summarise(mean_correct = mean(acc,na.rm = TRUE))

df.stim_acc <- merge(stimuli,acc_per_item,by.x="fullID",by.y="items")
df.stim_acc$acc <- round(df.stim_acc$acc,digits=2)
df.stim_acc$plaus <- round(df.stim_acc$plaus,digits=2)

items_reject <- df.stim_acc %>% arrange(Verb) %>% filter(acc < threshold)
                                                           
  #datatable(items_reject[,c(1,2,4,6:13,17)],
  #          filter = 'top',
  #          options = list("pageLength" = 10))
  datatable(items_reject[,c(1,2,4,6:13,17,18)])


```

```{r,echo=FALSE,warning=FALSE,message=FALSE}
###Sentences that have only around 50% expected responses
items_chance <- items_reject[items_reject$acc <= 0.6 & items_reject$acc >=0.4,]
pairs_chance <- df.stim_acc[df.stim_acc$ID %in% items_chance$ID,]  %>% arrange(ID)
#datatable(pairs_chance[,c(1,2,6:13,17)])
#It seems that for Type 2 items with low accuracy, that the semantics of N0 and N1 were both too related to N2, so that Verb attachment is preferred (high accuracy for verb attached sentences, low accuracy for noun attached sentences)
```
```{r,echo=FALSE,warning=FALSE,message=FALSE}
###Type 2 pairs that have only high accuracy (above 70%) across both Verb and Noun attached items
items_high <- df.stim_acc$ID[df.stim_acc$Type == "2" &  df.stim_acc$acc >= 0.7]
pairs_high <- items_high[duplicated(items_high)]
pairs_high <- df.stim_acc[df.stim_acc$ID %in% pairs_high,] %>% arrange(ID)
#datatable(pairs_high[,c(1,2,6:13,17)])

```
```{r,echo=FALSE}
##Items with very low performance (less than 40%) for both items in a pair
items_low <- df.stim_acc$ID[df.stim_acc$acc <= 0.4]
pairs_low <- items_low[duplicated(items_low)]
pairs_low <- df.stim_acc[df.stim_acc$ID %in% pairs_low,] %>% arrange(ID)
#datatable(pairs_low[,c(1,2,6:13,17)])

```
```{r,echo=FALSE}
##Pairs with very low performance (less than 40%) in one but high performance in paired item
pairs_onelow <- df.stim_acc[df.stim_acc$ID %in% items_low,] %>% arrange(ID)
#datatable(pairs_onelow[,c(1,2,6:13,17)])
```

### 2b. Remaining items:
```{r,warning=FALSE,message=FALSE}
#first reject items based on accuracy threshold with their paired items
pairs_reject <- df.stim_acc[df.stim_acc$ID %in% items_reject$ID,]
#for some 'repaired items there is a new sentence completing the pair
#for these items, we 1. only reject the 'bad' sentence and 2. relabel the others
#1.
pairs_reject <- pairs_reject[pairs_reject$fullID != "NA116",]
df.responses_clean <- df.responses %>%
                filter(!(items %in% pairs_reject$fullID))

#Then check if all verbs still occur twice (also for type 1 items) and reject remaining pair.
pairs_remain <- df.stim_acc[df.stim_acc$fullID %in% df.responses_clean$items,]
pairs_remain <-  unique(pairs_remain[duplicated(pairs_remain$verb_number),"verb_number"])
items_reject2 <- df.stim_acc[!(df.stim_acc$verb_number %in% pairs_remain),]
pairs_reject2 <- df.stim_acc[df.stim_acc$ID %in% items_reject2$ID,]
#1.
pairs_reject2 <- pairs_reject2[pairs_reject2$fullID != "NA116",]
pairs_reject2 <- pairs_reject2[pairs_reject2$fullID != "NA163",]
df.responses_clean <- df.responses_clean %>%
                filter(!(items %in% pairs_reject2$fullID))
#2.
levels(df.responses_clean$items) <- c(levels(df.responses_clean$items), "NA1110")
df.responses_clean$items[df.responses_clean$items == "NA116"] <- "NA1110"
df.responses_clean$items[df.responses_clean$items == "NA163"] <- "NA116"
df.responses_clean$items[df.responses_clean$items == "VA2105"] <- "VA116"
id <-"NA1110"
df.stim_acc$fullID[df.stim_acc$fullID == "NA116"] <- id
df.stim_acc$Pair[df.stim_acc$fullID == id] <- "110"
df.stim_acc$ID[df.stim_acc$fullID == id] <- "1110"
id <- "VA163"
df.stim_acc$fullID[df.stim_acc$fullID == "VA116"] <- id
df.stim_acc$Pair[df.stim_acc$fullID == id] <- "63"
df.stim_acc$ID[df.stim_acc$fullID == id] <- "163"
id <- "NA116"
df.stim_acc$fullID[df.stim_acc$fullID == "NA163"] <- id
df.stim_acc$Pair[df.stim_acc$fullID == id] <- "16"
df.stim_acc$ID[df.stim_acc$fullID == id] <- "116"
id <- "VA116"
df.stim_acc$fullID[df.stim_acc$fullID == "VA2105"] <- id
df.stim_acc$Pair[df.stim_acc$fullID == id] <- "16"
df.stim_acc$ID[df.stim_acc$fullID == id] <- "116"
df.stim_acc$Type[df.stim_acc$fullID == id] <- "1"

datatable(df.stim_acc[df.stim_acc$fullID %in% df.responses_clean$item,c(1,2,3,4,6:13,17,18)])

```


```{r}
#manually reject superfluous items
items_reject3 = c("VA249","NA249","NA21","VA21","NA243","VA243","NA2104","VA2104","NA247","VA247","NA288","VA288","NA255","VA255","NA237","VA237","NA22","VA22","NA25","VA25","NA24","VA24","NA272","VA272","NA229","VA229","NA286","VA286","NA287","VA287","NA275","VA275","NA278","VA278","NA253","VA253","NA120","VA120","NA133","VA133","NA125","VA125","NA183","VA183","NA188","VA188","NA1101","VA1101","NA147","VA147","NA1117","VA1117","NA1118","VA1118","VA1109")
df.responses_clean <- df.responses_clean %>%
                filter(!(items %in% items_reject3))
datatable(df.stim_acc[df.stim_acc$fullID %in% df.responses_clean$item,c(1,2,4,6:13,17,18)])

#save the cleaned data
save(df.stim_acc,df.responses_clean,df.subjectinfo,file = 'pretestdata_clean.RData')
```
There are `r nrow(df.stim_acc[df.stim_acc$fullID %in% df.responses_clean$items & df.stim_acc$Type==1,])` items of Type 1 left  
There are `r nrow(df.stim_acc[df.stim_acc$fullID %in% df.responses_clean$items & df.stim_acc$Type==2,])` items of Type 2 left

## 3. Biases
### 3a. Accuracy for Verb and Noun attached items
```{r echo=FALSE, message=FALSE, warning=FALSE}
summary_acc <- df.responses_clean %>%
                 group_by(subjects,attachment) %>%
                   summarise(mean_acc = mean(hits,na.rm = TRUE)) 
                      
p <- ggplot(summary_acc, aes(x = factor(attachment), y = mean_acc,subject = subjects,fill=factor(attachment))) +
    geom_boxplot() +
    geom_jitter(size = 2) +
    ggtitle("Accuracy averaged over all sentences") +
    labs(y="mean accuracy (%)", x="")+
    scale_x_discrete(labels = c("Noun-attached","Verb-attached")) + theme(legend.position = "none")
p <- ggplotly(p,tooltip = c("subject","mean_rt"))
p$x$data[1] <- lapply(p$x$data[1], FUN=function(x){
  x$marker = list(opacity = 0)
  return(x)
})
p$x$data[2] <- lapply(p$x$data[2], FUN=function(x){
  x$marker = list(opacity = 0)
  return(x)
})
p

#statistical test
#paired ttest since each observation is a subject and for each subject there is a mean accuracy for all noun-attached and all verb-attached sentences (although those means were calculated from different amounts of trials)
t.test(summary_acc$mean_acc[summary_acc$attachment=="Noun"],summary_acc$mean_acc[summary_acc$attachment=="Verb"],paired=TRUE)
```
### 3b. Average reaction times per subject, split for verb/noun and hit(1)/miss(0)
```{r message=FALSE, warning=FALSE}
summary_rts <- df.responses_clean %>%
                 group_by(subjects,attachment,hits) %>%
                   summarise(mean_rt = mean(rt_attachment)) 
                      
p <- ggplot(data = subset(summary_rts, !is.na(hits)), aes(x = factor(hits), y = mean_rt,subject = subjects,fill=factor(hits))) +
    geom_boxplot() +
    facet_wrap(~attachment) +
    geom_jitter(size = 2) +
    ggtitle("RTs averaged over items")+
    labs(y="mean reaction time (RT)", x="")+
    scale_x_discrete(labels = c("miss","hit","miss","hit")) +
    theme(legend.position = "none")
p <- ggplotly(p,tooltip = c("subject","mean_rt"))
p$x$data[1] <- lapply(p$x$data[1], FUN=function(x){
  x$marker = list(opacity = 0)
  return(x)
})
p$x$data[2] <- lapply(p$x$data[2], FUN=function(x){
  x$marker = list(opacity = 0)
  return(x)
})
p$x$data[3] <- lapply(p$x$data[3], FUN=function(x){
  x$marker = list(opacity = 0)
  return(x)
})
p
## stats
# Are reaction times significantly different between nouns and verbs?
data <- summary_rts <- df.responses_clean %>%
                 group_by(subjects,attachment) %>%
                   summarise(mean_rt = mean(rt_attachment,na.rm = TRUE)) 

t.test(data$mean_rt[data$attachment=="Noun"],data$mean_rt[data$attachment=="Verb"],paired=TRUE)
```
### 3c. Difference in acc or RT for Type 1 and Type 2 items
```{r,warning=FALSE,message=FALSE}
summary_acc <- df.responses_clean %>%
                 group_by(subjects,) %>%
                   summarise(mean_acc = mean(hits,na.rm = TRUE)) 
                      
p <- ggplot(summary_acc, aes(x = factor(attachment), y = mean_acc,subject = subjects,fill=factor(attachment))) +
    geom_boxplot() +
    geom_jitter(size = 2) +
    ggtitle("Accuracy averaged over all sentences") +
    labs(y="mean accuracy (%)", x="")+
    scale_x_discrete(labels = c("Noun-attached","Verb-attached")) + theme(legend.position = "none")
p <- ggplotly(p,tooltip = c("subject","mean_rt"))
p$x$data[1] <- lapply(p$x$data[1], FUN=function(x){
  x$marker = list(opacity = 0)
  return(x)
})
p$x$data[2] <- lapply(p$x$data[2], FUN=function(x){
  x$marker = list(opacity = 0)
  return(x)
})
p

#statistical test
#paired ttest since each observation is a subject and for each subject there is a mean accuracy for all noun-attached and all verb-attached sentences (although those means were calculated from different amounts of trials)
t.test(summary_acc$mean_acc[summary_acc$attachment=="Noun"],summary_acc$mean_acc[summary_acc$attachment=="Verb"],paired=TRUE)




acc_per_item <- df.responses_clean %>%
                  group_by(items) %>%
                    summarise(acc = mean(hits,na.rm = TRUE))
mean_accuracy <- acc_per_item %>%
                    summarise(mean_correct = mean(acc,na.rm = TRUE))

diff_acc <- df.stim_acc %>% group_by(ID) %>% arrange(ID) %>% mutate(diff_acc= acc -lag(acc))

p <- ggplot(diff_acc, aes(Type,diff_acc,colour=Type,N0=N0,Verb=Verb,N1=N1,Prep=Prep.,Adj=Adj.,N2=N2)) +
    geom_boxplot() +
    geom_jitter(size = 1) +
    ggtitle("Difference in accuracy within pairs: \n contrast Verb-attached vs. Noun-attached ")+
    labs(y="difference in accuracy (%)",colour="Type")+
    scale_x_discrete(labels = c("Type 1: differ in Verb","Type 2: differ in Noun order"))
p <- ggplotly(p,tooltip =c('N0','Verb','N1','Prep','Adj','N2'))

#let extra boxplot outlier dots disappear
p$x$data[1] <- lapply(p$x$data[1], FUN=function(x){
  x$marker = list(opacity = 0)
  return(x)
})
p$x$data[2] <- lapply(p$x$data[2], FUN=function(x){
  x$marker = list(opacity = 0)
  return(x)
})
#plot
p

## paired t-test
## t.test(diff_acc$diff_acc[diff_acc$Type==1],diff_acc$diff_acc[diff_acc$Type==2])
```

### 3d. Percentage of Verb responses per subject
```{r,warning=FALSE,message=FALSE}
attachment_freq <- df.responses_clean %>%
                        na.omit %>%
                          group_by(subjects,response_attachment) %>%
                            summarise(count=n())%>%
                            mutate(freq = count/sum(count))
                           #summarise(percent = length(response_attachment)/162) 
percent_VAresponses <- attachment_freq[attachment_freq$response_attachment=="Verb",]
percent_NAresponses <- attachment_freq[attachment_freq$response_attachment=="Nomen",]
#ggplot(attachment_percent, aes(percent, fill = response_attachment)) + 
#  geom_density(alpha = 0.2)
p <- ggplot(attachment_freq, aes(freq, fill = response_attachment)) + geom_histogram(alpha = 0.5, aes(y = ..density..), position = 'identity')
p <- ggplotly(p)
p
```

```{r echo=FALSE}
#t.test( percent_NAresponses$freq,percent_VAresponses$freq,paired=TRUE)
```

## 3e Mixed effects model
Questions to get answered by the model:
1) Is there a verb bias in the population and if so, is it caused through bias in my stimuli? That is are subjects more accurate on verb-attached sentences or faster at answering verb-attached sentences?

2) If subjects are neither more accurate nor faster to answer verb-attached sentences is there an interaction between those two factors? For example, are subjects more accurate in recognizing verb-attached sentences when they are answering fast but are more accurate in answering noun-attached sentences when answering slowly?
```{r,warning=FALSE,message=FALSE}

```



## 4. Analyse Plausibility ratings
```{r,warning=FALSE}
plausibility_counts <- df.responses_clean %>%
                        group_by(attachment) %>%
                        count(rating_plausibility) %>% na.omit()

ggplot(plausibility_counts,aes(rating_plausibility,n,fill=attachment))+
  geom_bar(stat="identity",position=position_dodge())
```

```{r,echo=FALSE,warning=FALSE,message=FALSE}
#reduce dimensionality
plausibility_counts <- cbind(plausibility_counts[plausibility_counts$attachment=="Noun",3],
                             plausibility_counts[plausibility_counts$attachment=="Verb",3])
colnames(plausibility_counts) <- c("Noun","Verb")
plausibility_counts <- rbind(c(sum(plausibility_counts[c(1,2),1]),
                                sum(plausibility_counts[c(1,2),2])),
                             c(sum(plausibility_counts[c(4,5),1]),
                                sum(plausibility_counts[c(4,5),2])))
colnames(plausibility_counts) <- c("Noun","Verb")
#chi-square test
#chisq.test(plausibility_counts)
#wilcox.test(rating_plausibility~attachment,data=df.responses)
library(MASS)
select <- dplyr::select
#ordinal logistic regression model
mod <- polr(as.factor(rating_plausibility)~attachment+rt_attachment+rt_plausibility,data=df.responses)
coeffs <- coef(summary(mod))
p <- pnorm(abs(coeffs[,"t value"]),lower.tail=FALSE)*2
#cbind(coeffs,"p value" = round(p,3))
```
### 4b. Reaction times & Plausibility:
```{r,warning=FALSE,message=FALSE}
# Group RTs by plausibility ratings and attachment
summary_rts <- df.responses_clean %>%
  group_by(items,attachment,rating_plausibility) %>%
  summarise(mean_rt = mean(rt_attachment)) # does make sense to take mean here?

p <- ggplot(summary_rts, aes(x = factor(rating_plausibility), y = mean_rt,items = items,fill=factor(rating_plausibility))) +
  geom_boxplot() +
  facet_wrap(~attachment) +
  ggtitle("RT distribution over items per plausibility bin")
p <- ggplotly(p,tooltip = c("items","mean_rt"))
p
```

