---
author: "Sophie Arana"
title: "Pre-test results exploration"
output:
  html_document:
    df_print: paged
    code_folding: hide
df_print: paged
#toc: yes
#toc_depth: 2
#toc_float: yes
---

```{r,echo=FALSE, message=FALSE}
library(plotly)
library(ggplot2)
library(reshape)
library(RColorBrewer)
library(data.table)
library(plyr)
library(dplyr)
library(kableExtra)
library(knitr)
options(knitr.table.format="html")
#Create color scheme
pastel_colors = brewer.pal(8, "Pastel1")
set_colors = brewer.pal(8, "Set1")
```

# Loading in data
All following graphs are based the results of the Limesurvey online pre-test. 40 participants filled out the online pre-test, of which 20 saw the same sentences and each participant saw them in a different pseudo-randomized order.

```{r}
stimuli         <- read.csv(file="stimuliB.csv",head=TRUE,sep=",",na.strings = c("","NAN"), stringsAsFactors = FALSE,encoding="UTF-8")
stimuli$fullID  <- as.character(interaction(stimuli[,c(13,3)],sep = ""))
#load response file
file_names = list.files(pattern="^results",path = "Bresponses",full.names = TRUE)
temp <- lapply(file_names,read.csv,sep=",",na.strings = c("","NAN"), stringsAsFactors = TRUE)
df.responses <- Reduce(function(x,y) merge(x,y,all=TRUE,sort=TRUE),temp)
#transpose and fix column names and classes
df.responses                <- setDT(df.responses,keep.rownames = TRUE)
colnames(df.responses)[1]   <- "subjects"
df.responses                <- Filter(function(x) !(all(x=="")), df.responses) #delete blank columns
```

Data was subsequently split into subject info (demographics etc.) and responses to items

```{r}
#Split all item specific data from subject specific data
ind_items   <- grep("[VN]A[0-9]",colnames(df.responses),value = TRUE)
df.subjectinfo <- df.responses
df.subjectinfo[,ind_items] <- NULL
df.responses  <- df.responses[,c("subjects",ind_items),with=FALSE]
```
```{r}
#reshape items so that each row contains info for items per subject (items repeat over rows)
ind             <- grep("^[VN]A[0-9]*$",colnames(df.responses),value = TRUE)
df.respAttach   <- df.responses[,c("subjects",ind),with=FALSE]
df.respAttach   <- melt(df.respAttach,id="subjects",value.name="response_attachment",variable.name='items')
ind             <- grep("^[VN]A[0-9]*Time$",colnames(df.responses),value = TRUE)
df.respTime     <- df.responses[,c("subjects",ind),with=FALSE]
df.respTime     <- melt(df.respTime,id="subjects",value.name="rt_attachment",variable.name='items')
ind             <- grep("\\.$",colnames(df.responses),value = TRUE)
df.respPAttach  <- df.responses[,c("subjects",ind),with=FALSE]
df.respPAttach  <- melt(df.respPAttach,id="subjects",value.name="rating_plausibility",variable.name='items')
ind             <- grep("P[VN]A[0-9]*Time$",colnames(df.responses),value = TRUE)
df.respPTime    <- df.responses[,c("subjects",ind),with=FALSE]
df.respPTime    <- melt(df.respPTime,id="subjects",value.name="rt_plausibility",variable.name='items')
df.responses    <- cbind(df.respAttach,df.respTime[,3],df.respPAttach[,3],df.respPTime[,3])
df.responses$hits <- as.numeric((grepl("N",df.responses$items) & df.responses$response_attachment == "Nomen") | 
                      (grepl("V",df.responses$items) & df.responses$response_attachment == "Verb"))
df.responses    <- df.responses %>%
                    mutate(attachment = ifelse(grepl("N",items),"Noun","Verb"))
                                                                       
```
#Summary Statistics
Age of participants:
```{r}
summary(df.subjectinfo$age)
```
Time taken to complete pre-test (in minutes):
```{r}
summary((df.subjectinfo$interviewtime)/60)
```
##Outlier
Outlier subjects were identified according to three scores:  
1. Subjects who have more than one incorrect test item  
2. Subjects who have less than 69% correct for the unambiguous items  
3. Subjects who's average reaction time diverges extremely from the average

Unambiguous sentences:
```{r}
#average accuracy of unambiguous items
subset_unambiguous <- stimuli$fullID[(stimuli$Unambiguous.==1)]
sentences_unambiguous <- stimuli[(stimuli$Unambiguous.==1),4:12]
sentences_unambiguous
```
  
Boxplot for average reaction times across subjects showing outliers:
```{r}
least_correct = 3
threshhold_acc = 0.69
avg_rts <- df.responses %>%
                        group_by(subjects) %>%
                        summarise(mean_rt = mean(rt_attachment)) 
boxplot(avg_rts[,2])
outlier <- boxplot(avg_rts[,2])$out

test_correct   <- df.subjectinfo %>%
                    select(subjects,starts_with("correct")) %>%
                      mutate(number_correct = rowSums(!is.na(.[,2:ncol(.)]))) %>%
                        select(subjects,number_correct)

avg_unambiguous <-df.responses %>%
                    filter(items %in% subset_unambiguous) %>%
                      group_by(subjects) %>%
                        summarise(accuracy_unambiguous = mean(hits))

summ <- join_all(list(avg_rts,test_correct,avg_unambiguous),by='subjects')
summ

```
```{r}
outlier_indx <- avg_rts$subjects[which(round(avg_rts$mean_rt,digits=2) %in% round(outlier,digits=2))]
outlier_indx <- c(outlier_indx,summ$subjects[which(summ$number_correct < least_correct | summ$accuracy < threshhold_acc)])
#Remove Outlier
df.responses <- df.responses %>%
                filter(!(subjects %in% outlier_indx))
```
  
Number of removed outliers: `r length(outlier_indx)`

# Is there a Verb bias?
##Average reaction times per subject, split for verb/noun and hit(1)/miss(0)
```{r message=FALSE}
summary_rts <- df.responses %>%
                 group_by(subjects,attachment,hits) %>%
                   summarise(mean_rt = mean(rt_attachment)) 
                      
p <- ggplot(summary_rts, aes(x = factor(hits), y = mean_rt,subject = subjects,fill=factor(hits))) +
    geom_boxplot() +
    facet_wrap(~attachment) +
    geom_jitter(size = 2) +
    ggtitle("RTs averaged over items")+
    scale_x_discrete(labels = c("miss","hit","miss","hit"))
p <- ggplotly(p,tooltip = c("subject","mean_rt"))
p
```

Percentage of Verb responses per subject
```{r}
attachment_percent <- df.responses %>%
  group_by(subjects,response_attachment) %>%
    summarise(percent = length(response_attachment)/162) 
plot(attachment_percent[attachment_percent$response_attachment=="Verb",c(1,3)])
```

```{r echo=FALSE}
t.test( attachment_percent$percent[attachment_percent$response_attachment=="Nomen"], attachment_percent$percent[attachment_percent$response_attachment=="Verb"],paired=TRUE)
```

#Assess items
Binomial distribution of probabilities if chance-performane is 50%
```{r}
x <- seq(0,20,by = 1)
y <- dbinom(x,20,0.5)
plot(x,y)
```
Reject stimuli with less than 75% correct responses
```{r}
threshold = 0.75
mean_accuracy <- df.responses %>%
                    group_by(items) %>%
                      summarise(percent_correct = round(sum(hits)/length(file_names),digits=2)) %>%
                        summarise(mean_correct = mean(percent_correct))
acc_per_item <- df.responses %>%
                  group_by(items) %>%
                    summarise(percent_correct = round(sum(hits)/length(file_names),digits=2))


items_reject  <- df.responses %>%
                  group_by(items) %>%
                    summarise(percent_correct = round(sum(hits)/length(file_names),digits=2)) %>%
                     filter(percent_correct <= threshold)
stimuli_reject <- stimuli[(stimuli$fullID %in% items_reject$items),c(4:12,15)]

```
Leads to rejection of `r nrow(stimuli_reject)` sentences  
Of which `r length(grep("^[VN]A1",stimuli_reject$fullID))` are Type 1 items.  
And `r length(grep("^[VN]A2",stimuli_reject$fullID))` are Type 2 items.
```{r}
out <- merge(stimuli_reject,items_reject,by.x = "fullID",by.y = "items")
out %>% arrange(Verb)
```

#Analyse Plausibility ratings
```{r}
plausibility_counts <- df.responses %>%
                        group_by(attachment) %>%
                        count(rating_plausibility)
plausibility_counts <- cbind(plausibility_counts[plausibility_counts$attachment=="Noun",3],
                             plausibility_counts[plausibility_counts$attachment=="Verb",3])
colnames(plausibility_counts) <- c("Noun","Verb")
#reduce dimensionality
plausibility_counts <- rbind(c(sum(plausibility_counts[c(1,2),1]),
                                sum(plausibility_counts[c(1,2),2])),
                             c(sum(plausibility_counts[c(4,5),1]),
                                sum(plausibility_counts[c(4,5),2])))
#chi-square test
chisq.test(plausibility_counts)
```
Reaction times & Plausibility:
```{r}
# Group RTs by plausibility ratings and attachment
summary_rts <- df.responses %>%
  group_by(items,attachment,rating_plausibility) %>%
  summarise(mean_rt = mean(rt_attachment)) # does make sense to take mean here?

p <- ggplot(summary_rts, aes(x = factor(rating_plausibility), y = mean_rt,items = items,fill=factor(rating_plausibility))) +
  geom_boxplot() +
  facet_wrap(~attachment) +
  ggtitle("RT distribution over items per plausibility bin")
p <- ggplotly(p,tooltip = c("items","mean_rt"))
p
```

